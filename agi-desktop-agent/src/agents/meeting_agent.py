"""
Meeting Preparation Agent
Scenario 3: Meeting Preparation Assistant

ReAct pattern: Observe (memory) â†’ Reason (plan) â†’ Act (Linkup search) â†’ Synthesize (Groq briefing)
"""

import json
from datetime import datetime


class MeetingAgent:
    """
    Generates comprehensive meeting briefings by combining:
    1. Local memory (past emails & meetings)
    2. Real-time web research via Linkup
    3. LLM synthesis via Groq (Llama 3.3 70B)
    """

    def __init__(self, groq_client, linkup_wrapper, memory_store):
        self.groq = groq_client
        self.linkup = linkup_wrapper
        self.memory = memory_store

    def prepare_briefing(self, company_name: str, meeting_context: str = "") -> dict:
        """
        Main entry point â€” generates a full meeting briefing.

        Args:
            company_name: Name of the company for the meeting
            meeting_context: Optional context (e.g. "Partnership discussion")

        Returns:
            {
                'reasoning_steps': [...],
                'past_interactions': {...},
                'linkup_sources': [...],
                'result': str,          # The formatted briefing
                'briefing_data': {      # Structured data for UI
                    'company_overview': str,
                    'past_context': str,
                    'recent_news': str,
                    'talking_points': [str],
                    'risks_and_notes': str
                },
                'confidence': float
            }
        """
        reasoning_steps = []
        all_sources = []

        # â”€â”€ Step 1: Search local memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        reasoning_steps.append({
            'step': 1,
            'action': 'Searching local memory',
            'detail': f'Looking for past emails and meetings with {company_name}...',
            'status': 'running'
        })

        context = self.memory.get_company_context(company_name)
        total = context['total_interactions']
        last = context['last_contact']

        reasoning_steps[-1]['status'] = 'complete'
        reasoning_steps[-1]['detail'] = (
            f"Found {total} past interaction(s). "
            f"Last contact: {last or 'Never'}."
        )

        # â”€â”€ Step 2: Linkup research â€” recent news â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        reasoning_steps.append({
            'step': 2,
            'action': 'Researching recent news',
            'detail': f'Searching Linkup for latest {company_name} news...',
            'status': 'running'
        })

        news_results = self._search_news(company_name)
        all_sources.extend(news_results.get('sources', []))

        reasoning_steps[-1]['status'] = 'complete'
        reasoning_steps[-1]['detail'] = (
            f"Found {len(news_results.get('sources', []))} news source(s) "
            f"about {company_name}."
        )

        # â”€â”€ Step 3: Linkup research â€” industry / products â”€â”€â”€â”€â”€â”€â”€â”€
        reasoning_steps.append({
            'step': 3,
            'action': 'Researching products & funding',
            'detail': f'Searching Linkup for {company_name} products, funding, strategy...',
            'status': 'running'
        })

        industry_results = self._search_industry(company_name)
        all_sources.extend(industry_results.get('sources', []))

        reasoning_steps[-1]['status'] = 'complete'
        reasoning_steps[-1]['detail'] = (
            f"Found {len(industry_results.get('sources', []))} additional source(s)."
        )

        # â”€â”€ Step 4: Synthesize briefing with Groq â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        reasoning_steps.append({
            'step': 4,
            'action': 'Generating briefing',
            'detail': 'Synthesizing all data into a comprehensive meeting brief...',
            'status': 'running'
        })

        briefing_result = self._synthesize_briefing(
            company_name=company_name,
            meeting_context=meeting_context,
            memory_context=context,
            news_sources=news_results,
            industry_sources=industry_results
        )

        reasoning_steps[-1]['status'] = 'complete'
        reasoning_steps[-1]['detail'] = 'Briefing generated successfully.'

        # â”€â”€ Step 5: Note completion (no longer saving duplicate records) â”€
        reasoning_steps.append({
            'step': 5,
            'action': 'Briefing complete',
            'detail': f'Compiled data from {total} past interaction(s) and {len(all_sources)} web source(s).',
            'status': 'complete'
        })

        # â”€â”€ Build result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        confidence = self._calculate_confidence(context, all_sources, briefing_result)

        # â”€â”€ Source attribution analytics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        source_attribution = self._calculate_source_attribution(context, all_sources, briefing_result)

        return {
            'reasoning_steps': reasoning_steps,
            'past_interactions': {
                'emails': context['emails'],
                'conversations': context['conversations'],
                'total': context['total_interactions'],
                'last_contact': context['last_contact'],
                'summary': context['summary']
            },
            'linkup_sources': all_sources,
            'result': briefing_result.get('briefing_text', 'Unable to generate briefing.'),
            'briefing_data': briefing_result.get('briefing_data', {}),
            'confidence': confidence,
            'source_attribution': source_attribution
        }

    # â”€â”€ Linkup search helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _search_news(self, company: str) -> dict:
        """Search Linkup for recent company news."""
        try:
            return self.linkup.search(
                query=f"{company} recent news announcements 2025 2026",
                max_results=4
            )
        except Exception as e:
            print(f"[MeetingAgent] News search error: {e}")
            return {'sources': [], 'error': str(e)}

    def _search_industry(self, company: str) -> dict:
        """Search Linkup for company products, funding, strategy."""
        try:
            return self.linkup.search(
                query=f"{company} products funding strategy leadership",
                max_results=4
            )
        except Exception as e:
            print(f"[MeetingAgent] Industry search error: {e}")
            return {'sources': [], 'error': str(e)}

    # â”€â”€ Groq synthesis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _synthesize_briefing(
        self,
        company_name: str,
        meeting_context: str,
        memory_context: dict,
        news_sources: dict,
        industry_sources: dict
    ) -> dict:
        """Use Groq (Llama 3.3) to generate a structured meeting briefing."""

        # Build the context text for the LLM
        news_text = self.linkup.format_sources_for_agent(
            news_sources.get('sources', [])
        )
        industry_text = self.linkup.format_sources_for_agent(
            industry_sources.get('sources', [])
        )

        system_prompt = """You are an expert executive assistant AI. Your job is to create 
comprehensive, actionable meeting preparation briefings.

You must output a JSON object with these exact keys:
{
  "company_overview": "2-3 sentence overview of the company",
  "past_context": "Summary of past interactions and relationship status",
  "recent_news": "Key recent developments, funding, product launches",
  "talking_points": ["point 1", "point 2", "point 3", "point 4"],
  "risks_and_notes": "Anything to be cautious about or prepare for"
}

Be specific, actionable, and concise. Reference real data from the sources provided.
Output ONLY valid JSON, no markdown or extra text."""

        user_message = f"""Prepare a meeting briefing for a meeting with **{company_name}**.

Meeting context: {meeting_context or 'General business meeting'}

=== PAST INTERACTIONS (from our records) ===
{memory_context['summary']}

=== RECENT NEWS (from web research) ===
{news_text if news_text.strip() else 'No recent news found.'}

=== INDUSTRY & PRODUCT INFO (from web research) ===
{industry_text if industry_text.strip() else 'No industry info found.'}

Generate the briefing JSON now."""

        try:
            response = self.groq.chat(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.4  # Lower temp for factual output
            )

            content = response.get('content', '')
            if not content:
                return self._fallback_briefing(company_name, memory_context)

            # Parse the JSON from LLM response
            briefing_data = self._parse_briefing_json(content)

            # Build the human-readable text version
            briefing_text = self._format_briefing_text(company_name, briefing_data)

            return {
                'briefing_text': briefing_text,
                'briefing_data': briefing_data
            }

        except Exception as e:
            print(f"[MeetingAgent] Groq synthesis error: {e}")
            return self._fallback_briefing(company_name, memory_context)

    def _parse_briefing_json(self, content: str) -> dict:
        """Extract JSON from LLM response, handling markdown code fences."""
        # Strip markdown code fences if present
        cleaned = content.strip()
        if cleaned.startswith('```'):
            lines = cleaned.split('\n')
            # Remove first and last lines (fences)
            lines = [l for l in lines if not l.strip().startswith('```')]
            cleaned = '\n'.join(lines)

        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            # Try to find JSON object in the text
            start = cleaned.find('{')
            end = cleaned.rfind('}')
            if start != -1 and end != -1:
                try:
                    return json.loads(cleaned[start:end + 1])
                except json.JSONDecodeError:
                    pass
            return {
                'company_overview': 'Unable to parse briefing.',
                'past_context': content[:200],
                'recent_news': '',
                'talking_points': ['Review the raw response above'],
                'risks_and_notes': ''
            }

    def _format_briefing_text(self, company: str, data: dict) -> str:
        """Format briefing data into readable text."""
        points = data.get('talking_points', [])
        talking = '\n'.join(f"  {i+1}. {p}" for i, p in enumerate(points))

        return f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  MEETING BRIEFING: {company.upper()}
  Prepared: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“‹ COMPANY OVERVIEW
{data.get('company_overview', 'N/A')}

ðŸ”„ PAST INTERACTIONS
{data.get('past_context', 'No past interactions found.')}

ðŸ“° RECENT NEWS & DEVELOPMENTS
{data.get('recent_news', 'No recent news available.')}

ðŸ’¬ RECOMMENDED TALKING POINTS
{talking or '  No talking points generated.'}

âš ï¸ RISKS & NOTES
{data.get('risks_and_notes', 'None identified.')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""".strip()

    def _fallback_briefing(self, company: str, memory_context: dict) -> dict:
        """Generate a basic briefing when Groq is unavailable."""
        data = {
            'company_overview': f'Meeting scheduled with {company}.',
            'past_context': memory_context.get('summary', 'No past data.'),
            'recent_news': 'Unable to fetch â€” check Linkup API key.',
            'talking_points': [
                'Review past interaction history',
                'Ask about their current priorities',
                'Explore mutual opportunities',
                'Discuss timeline and next steps'
            ],
            'risks_and_notes': 'Briefing generated with limited data. Verify details manually.'
        }
        return {
            'briefing_text': self._format_briefing_text(company, data),
            'briefing_data': data
        }

    # â”€â”€ Confidence scoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _calculate_confidence(self, context: dict, sources: list, briefing: dict) -> float:
        """
        Confidence = weighted score of data availability.
        Gives judges a measurable quality indicator.
        """
        score = 0.0
        # Memory data found
        if context['total_interactions'] > 0:
            score += 0.3
        # Linkup sources found
        if len(sources) >= 3:
            score += 0.3
        elif len(sources) >= 1:
            score += 0.15
        # Briefing generated successfully
        if briefing.get('briefing_data', {}).get('talking_points'):
            score += 0.3
        # Has recent contact
        if context.get('last_contact'):
            score += 0.1
        return min(round(score, 2), 1.0)

    def _calculate_source_attribution(self, context: dict, sources: list, briefing: dict) -> dict:
        """
        Calculate what percentage of the briefing came from each source.
        Returns breakdown for the analytics panel.
        """
        has_memory = context['total_interactions'] > 0
        has_linkup = len(sources) > 0
        has_briefing = bool(briefing.get('briefing_data'))

        # Base weights depending on what data was available
        memory_weight = 0.0
        linkup_weight = 0.0
        llm_weight = 0.0

        if has_memory and has_linkup:
            memory_weight = 30.0
            linkup_weight = 40.0
            llm_weight = 30.0
        elif has_memory and not has_linkup:
            memory_weight = 50.0
            linkup_weight = 0.0
            llm_weight = 50.0
        elif not has_memory and has_linkup:
            memory_weight = 0.0
            linkup_weight = 55.0
            llm_weight = 45.0
        else:
            memory_weight = 0.0
            linkup_weight = 0.0
            llm_weight = 100.0

        # Adjust memory weight based on how many interactions found
        if has_memory:
            interaction_bonus = min(context['total_interactions'] * 3, 10)
            memory_weight += interaction_bonus
            llm_weight -= interaction_bonus

        # Adjust linkup weight based on number of sources
        if has_linkup:
            source_bonus = min(len(sources) * 2, 10)
            linkup_weight += source_bonus
            llm_weight -= source_bonus

        # Ensure non-negative and normalize to 100
        llm_weight = max(llm_weight, 5.0)
        total = memory_weight + linkup_weight + llm_weight
        memory_pct = round(memory_weight / total * 100)
        linkup_pct = round(linkup_weight / total * 100)
        llm_pct = 100 - memory_pct - linkup_pct  # Ensure sums to 100

        # Section-level attribution
        sections = {}
        bd = briefing.get('briefing_data', {})
        if bd:
            sections['company_overview'] = {
                'primary_source': 'Web Research (Linkup)' if has_linkup else 'LLM Knowledge',
                'sources_used': (['Linkup'] if has_linkup else []) + ['LLM']
            }
            sections['past_context'] = {
                'primary_source': 'Local Memory' if has_memory else 'LLM Knowledge',
                'sources_used': (['Local Memory'] if has_memory else []) + ['LLM']
            }
            sections['recent_news'] = {
                'primary_source': 'Web Research (Linkup)' if has_linkup else 'LLM Knowledge',
                'sources_used': (['Linkup'] if has_linkup else []) + ['LLM']
            }
            sections['talking_points'] = {
                'primary_source': 'LLM Synthesis',
                'sources_used': (['Local Memory'] if has_memory else []) + (['Linkup'] if has_linkup else []) + ['LLM']
            }
            sections['risks_and_notes'] = {
                'primary_source': 'LLM Analysis',
                'sources_used': (['Local Memory'] if has_memory else []) + (['Linkup'] if has_linkup else []) + ['LLM']
            }

        return {
            'memory_pct': memory_pct,
            'linkup_pct': linkup_pct,
            'llm_pct': llm_pct,
            'memory_interactions': context['total_interactions'],
            'linkup_sources_count': len(sources),
            'data_freshness': 'Real-time' if has_linkup else 'Cached only',
            'sections': sections,
            'verification_status': 'Cross-verified' if (has_memory and has_linkup) else 'Partially verified' if (has_memory or has_linkup) else 'Unverified'
        }


# Quick test
if __name__ == "__main__":
    from groq_client import GroqClient
    from linkup_wrapper import LinkupWrapper
    import sys
    sys.path.insert(0, '..')
    from memory.store import MemoryStore

    agent = MeetingAgent(
        groq_client=GroqClient(),
        linkup_wrapper=LinkupWrapper(),
        memory_store=MemoryStore()
    )

    result = agent.prepare_briefing(
        company_name="TechCorp",
        meeting_context="Partnership discussion for AI integration"
    )

    print(f"Confidence: {result['confidence']}")
    print(f"Steps: {len(result['reasoning_steps'])}")
    print(f"Sources: {len(result['linkup_sources'])}")
    print(result['result'])
